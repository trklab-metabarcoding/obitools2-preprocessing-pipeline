---
title: "Step1c_data_processing"
subtitle: "Merge forward and reverse reads, dereplicate sequence reads across samples, and remove control and bad samples."
author: "T. Divoll & B.Littleford-Colquhoun"
date: "2024-10-29"
output: 
  html_document:
  df_print: paged
  toc: true
params:
    #don't change these
    user: !r Sys.getenv("LOGNAME")
    data_path: "/oscar/data/tkartzin/projects"
    todaydate: !r (format(Sys.Date(), "%Y%m%d"))
  
    #do change params below this line as needed
    project_code: "test"
    sequencing_date: "20221013" # edit this date to match the sequencing date of the run you are working on (i.e. date should match directory label)
    matching_score: 40 # --score-min in illuminapairedend
---
```{r include=FALSE}
## Only needed for knitr to render this notebook
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, here, tidyr, dplyr, readxl, stringr, spgs, filesstrings)
here::i_am("./Step1c_data_processing.Rmd")
```

```{r setup, include=FALSE}
# set global chunk parameters here
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
do.call('Sys.setenv', params)
```

#### Set up Conda env for OBITools
```{bash, install obitools, eval = FALSE, echo = FALSE}
source ~/.bash_profile

if conda info --envs | grep -q obitools-env
then 
  echo "obitools-env already exists" 
else 
  conda create -n obitools-env -y -c conda-forge -c bioconda obitools ecoprimers ecopcr python=2.7 
fi
```

#### Merge Illumina forward and reverse reads
```{bash}
#Recover full sequence reads from forward and reverse partial reads using illuminapairedend command in obitools
source ~/.bash_profile
source activate /users/${user}/.conda/envs/obitools-env
cd ./results

for sample in $(cat ../renamed_data/sampleNames)
do
    echo "On sample: $sample"
    
    illuminapairedend --score-min=$matching_score -r ${sample}_L001_R2_001_trimmed.fastq.gz ${sample}_L001_R1_001_trimmed.fastq.gz > ${sample}_merged.fastq
    
    obicount ${sample}_merged.fastq
done | column -t
```

#### Filter out unaligned reads
```{bash}
source ~/.bash_profile
source activate /users/${user}/.conda/envs/obitools-env
cd ./results

for sample in $(cat ../renamed_data/sampleNames)

do
    obigrep -p 'mode!="joined"' ${sample}_merged.fastq > ${sample}_ali.fastq
done
```

#### Add sample names to each file
```{bash}
source ~/.bash_profile
source activate /users/${user}/.conda/envs/obitools-env
cd ./results

for sample in $(cat ../renamed_data/sampleNames)

do
    obiannotate -S sample:${sample} ${sample}_ali.fastq >  ${sample}_id.fastq
    
done

mv *_id.fastq ./id_data
```

#### Inspect read counts in fecal samples vs controls
*Each time you run this script:* work with output 'sequencingdate_step1c_sample_readcount.txt' to identify "bad samples" and make sure sample/control read counts are in appropriate range. For manuscript reporting, make sure to include "bad samples" in summary counts.

*If your project includes multiple runs:* assuming consistency across runs, it's appropriate to combine values from each of these outputs to report summary counts in your paper for samples and controls. 

```{bash}
source ~/.bash_profile
source activate /users/${user}/.conda/envs/obitools-env
cd ./results/id_data

obistat -c sample *_id.fastq --without-progress-bar | column -t | tee -a ${sequencing_date}_step1c_sample_readcount.txt
```

#### Combine remaining sequences into one FASTA and dereplicate
```{bash quiet=TRUE}
source ~/.bash_profile
source activate /users/${user}/.conda/envs/obitools-env
cd ./results/id_data

obiuniq -m sample *_id.fastq --without-progress-bar > all.uniq.fasta

read number_of_asv total_sequences < <(obicount all.uniq.fasta --without-progress-bar)

echo $number_of_asv
echo $total_sequences
```

#### Filter results to just `count` and `merged_sample` attributes in FASTA headers
```{bash}
source ~/.bash_profile
source activate /users/${user}/.conda/envs/obitools-env
cd ./results/id_data

obiannotate -k count -k merged_sample all.uniq.fasta > $$ ; mv $$ all.uniq.fasta
```

#### Inspect the `count` attribute
```{bash}
source ~/.bash_profile
source activate /users/${user}/.conda/envs/obitools-env
cd ./results/id_data

# get the counting statistics on the ‘count’ attribute
obistat -c count all.uniq.fasta --without-progress-bar | sort -nk1 | head -20 | column -t | tee -a seq_count_table.txt
```

#### Create a tabulated file for counts per sequence read
```{bash}
source ~/.bash_profile
source activate /users/${user}/.conda/envs/obitools-env
cd ./results/id_data

obitab -o all.uniq.fasta > all.uniq.tab
```

#### Read the tab file as a table in R
After running the cell below, open the table from the Environment tab and inspect the results!
```{r}
all_uniq_tab <- read.delim(here("results/id_data/all.uniq.tab"), header=TRUE)
```

Use this space to take notes on the controls that have been looked into. For example, something like this: 
```
BLA20220920.A
6671/17628 (38%) – Glehnia littoralis = American silvertop
3442/5438 (63%) – Musa balbisiana = plantain
```

#### Move all the control samples and "bad" samples over to an archive directory
Now that the controls have been looked into, we can move the sample files over to a folder called `control_samples`
```{r}
merged_files <- list.files(here("results/id_data"), pattern="*id.fastq", full.names = T)
```

```{r}
control_files <- merged_files[grepl(paste(control_samples, collapse= "|"), merged_files)]
```

Add all of the `*id.fastq` files you want to move to the list named `bad_samples_to_move`. This will move them to a `bad_controls_samples` folder. These can be from suspected contamination or from low read counts (i.e., < 1000 reads). Make sure you write the full file name for each sample you want to remove. 
```{r}
##EXAMPLE: here("results/id_data", c("YNP218_S1_id.fastq", "YNP297_S13_id.fastq")) 

bad_samples_to_move <- here("results/id_data", c("", ""))
```

```{r}
move_files(control_files, here("results/bad_controls_path"), overwrite = FALSE)
move_files(bad_samples_to_move, here("results/bad_controls_path"), overwrite = FALSE)
```

Let's also move this first data set to an archive and we will re-run merging and de-replication in the next notebook, without controls and bad samples.
```{r}
move_files(c(here("results/id_data", "all.uniq.fasta"), here("results/id_data", "all.uniq.tab")), here("results/bad_controls_path"), overwrite = FALSE)
```

Create an environment variable for the results destination path.
```{r}
DIR = basename(getwd())
mapped_folder <- 
  filter(seq_date_names, value == DIR)$Folder
Sys.setenv(mapped_folder=mapped_folder)
Sys.setenv(analysis_date=params$todaydate)
```

Copy over your results folder and the Steps 1b and 1c notebooks to the shared lab directory on Oscar.
*Note: Save this notebook first so any results are recorded before copying.*
```{bash}
source ~/.bash_profile
source activate /users/${user}/.conda/envs/obitools-env

destination=${data_path}/${project_code}/${mapped_folder}/${DIR}/${analysis_date}_${user}
mkdir -p ${destination}
cp -r ./results/id_data ${destination}
cp -r ./results/bad_controls_path ${destination}
cp -r ./renamed_data ${destination}
cp *.Rmd ${destination}
cp ./results/cutadapt_primer_trimming_stats.txt ${destination}

conda deactivate
```

Now navigate to the notebook `Step3a_data_cleaning.Rmd` in the `obitools2-taxonomy-assignment` [repository](https://github.com/trklab-metabarcoding/obitools2-taxonomy-assignment). Use the Files pane in RStudio to navigate to `/oscar/data/tkartzin/r'params$user'/obitools2-taxonomy-assignment`.